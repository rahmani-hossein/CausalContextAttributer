{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class Prompt_Generator():\n",
    "\n",
    "\n",
    "    def __init__(self, prompt, LLM_Handler, num_datasets = 5 , pval=np.array([0.2, 0.4, 0.6, 0.8])):\n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "        self.original_prompt : original prompt to create subsets\n",
    "        self. pval : distribution P that p is drawn from. Normally  pval=[0.2, 0.4, 0.6, 0.8]\n",
    "        self.num_datasets : Number of datasets or subsets (M in the code) \n",
    "        self.prompts : a list of prompts that are our datasets.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_prompt = prompt\n",
    "        self. pval = pval\n",
    "        self.num_datasets = num_datasets\n",
    "        self.sample_prompts = []\n",
    "        self.LLM_Handler = LLM_Handler\n",
    "        # self.prompts.append(self.original_prompt) the original prompt itself isn't something special.\n",
    "\n",
    "\n",
    "    def sample_p(self, size=None):\n",
    "        return np.random.choice(self.pval, size=size)\n",
    "\n",
    "\n",
    "    def coef_scaling(self):\n",
    "        return (1/(self.pval *(1-self.pval))).mean()\n",
    "\n",
    "    def build_vocabulary(self, originalprompt):\n",
    "        \"\"\"\n",
    "        parameter : original prompt\n",
    "\n",
    "        return:\n",
    "        word_to_index : a dic to map words to index inthe original prompt.\n",
    "        unique_words : unique words in the original prompt. If a word is repetitive, the first index will be saved for word_to_index.\n",
    "        N : number of unique words  \n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', originalprompt)\n",
    "        # Step 2: Build the vocabulary\n",
    "        unique_words = []\n",
    "        word_to_index = {}\n",
    "        for word in words:\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(unique_words)\n",
    "                unique_words.append(word)\n",
    "\n",
    "        N = len(unique_words)\n",
    "        return word_to_index, unique_words, N\n",
    "    \n",
    "\n",
    "# To Do: resolve the bug that chatgpt doesn't produce exactly that much words.\n",
    "    def create_baseline_words(self):\n",
    "        word_to_index, _, N = self.build_vocabulary(self.original_prompt)\n",
    "        baseline_list = [[] for _ in range(N)]\n",
    "        rng = np.random.default_rng()\n",
    "        num_iterations = 5   #100 *N\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            p = np.random.choice(np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "            # print(p)\n",
    "            modified_prompt = remove_words_with_probability(self.original_prompt, probability=p)\n",
    "            # print(modified_prompt)\n",
    "            # Get the LLM to fill in the blanks\n",
    "            filled_prompt = self.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "            # print(\"filled prompt: \", filled_prompt)\n",
    "            filled_words = re.findall(r'\\b\\w+\\b', filled_prompt)\n",
    "\n",
    "        \n",
    "            modified_words = word_pattern.findall(modified_prompt)\n",
    "            for idx, token in enumerate(modified_words):\n",
    "                if token == '__':\n",
    "                    # print(\"fghj\",filled_words[idx])\n",
    "                    baseline_list[idx].append(filled_words[idx])\n",
    "\n",
    "\n",
    "        \n",
    "        return baseline_list\n",
    "\n",
    "\n",
    "\n",
    "#To Do : might change even before p-featurization\n",
    "    def create_X(self, method='uncompleted'):\n",
    "        self.ps = []\n",
    "        word_to_index, unique_words, N  = self.build_vocabulary(self.original_prompt)\n",
    "        X = np.zeros((self.num_datasets, N)) \n",
    "        nu = self.coef_scaling() # variance of each feature\n",
    "        for m in range(self.num_datasets):\n",
    "            p = self.sample_p()\n",
    "            self.ps.append(p)\n",
    "            modified_prompt = remove_words_with_probability(self.original_prompt, probability= p)\n",
    "            X[m,:] = -1/((1-p)* np.sqrt(nu))\n",
    "            modified_promptWords = re.findall(r'\\b\\w+\\b', modified_prompt)\n",
    "            for modified_word in modified_promptWords:\n",
    "                if modified_word !=\"__\":\n",
    "                    # print(\"we change probability of word: \", modified_word, \"in dataset \", m, \" with index \", word_to_index[modified_word])\n",
    "                    X[m,word_to_index[modified_word]] = 1/(p* np.sqrt(nu))\n",
    "            \n",
    "            # completed_prompt = self.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "            self.sample_prompts.append(modified_prompt)\n",
    "        return X\n",
    "    \n",
    "    # To Do sample from baseline for each word for putting typical word in it.\n",
    "    def fill_prompt_form_baseline(self, prompts):\n",
    "\n",
    "        return prompts\n",
    "    \n",
    "    \n",
    "    \n",
    "    # To do: add the baseline method. Here is just uncompleted.\n",
    "    def create_y(self, prompts, method='uncomplete'):\n",
    "        CLASSIFICATION_PROMPT_Base = \"\"\"You will be given a headline of a news article.\n",
    "        Classify the article into one of the following categories: Technology, Politics, Sports, Art or others.\n",
    "        Return only the name of the category, and nothing else.\n",
    "        MAKE SURE your output is one of the four categories stated.\n",
    "        Article headline: {prompt}\"\"\"\n",
    "\n",
    "        CLASSIFICATION_PROMPT_Method2 = \"\"\"You will be given a headline of a news article with some blanks instead of words.\n",
    "        Classify the article into one of the following categories: Technology, Politics, Sports, Art or others.\n",
    "        Return only the name of the category, and nothing else.\n",
    "        MAKE SURE your output is one of the four categories stated.\n",
    "        Article headline: {prompt}\"\"\"\n",
    "\n",
    "        if method == 'baseline':\n",
    "            baseline_list = self.create_baseline_words()\n",
    "            self.sample_prompts = self.fill_prompt_form_baseline(prompts) #TO Do\n",
    "\n",
    "        y = np.zeros(len(prompts))\n",
    "        outputs = []\n",
    "        for i in range(len(prompts)):\n",
    "            if method=='baseline':\n",
    "                API_RESPONSE = self.LLM_Handler.get_completion(\n",
    "                [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT_Base.format(prompt=prompts[i])}],\n",
    "                model=\"gpt-4\",\n",
    "                logprobs=True,\n",
    "                top_logprobs=1,\n",
    "                )\n",
    "            elif method =='uncomplete':\n",
    "                API_RESPONSE = self.LLM_Handler.get_completion(\n",
    "                [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT_Method2.format(prompt=prompts[i])}],\n",
    "                model=\"gpt-4\",\n",
    "                logprobs=True,\n",
    "                top_logprobs=1,\n",
    "                )\n",
    "\n",
    "\n",
    "            \n",
    "            val = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs[0]\n",
    "            outputs.append(val.token)\n",
    "            y[i] = val.logprob\n",
    "\n",
    "        return y, outputs\n",
    "        \n",
    "    \n",
    "\n",
    "          \n",
    "# Pre-compile regex patterns for efficiency\n",
    "\"\"\"\n",
    "word_pattern matches complete words composed of alphanumeric characters.\n",
    "token_pattern splits the prompt into words, whitespace, and punctuation, ensuring that all characters\n",
    " (including spaces, punctuation like commas and periods, and newlines) are preserved.\n",
    "\"\"\"\n",
    "word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "token_pattern = re.compile(r'\\b\\w+\\b|\\s+|[^\\w\\s]')\n",
    "\n",
    "def remove_words_with_probability(prompt, probability=0.8):\n",
    "    tokens = token_pattern.findall(prompt)\n",
    "    modified_tokens = [\n",
    "        '__' if word_pattern.fullmatch(token) and np.random.uniform(0,1) > probability else token\n",
    "        for token in tokens\n",
    "    ]\n",
    "    return ''.join(modified_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM_Handler():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv('config.env')\n",
    "        openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    \n",
    "    def fill_in_blanks(self, prompt):\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Please fill in the blanks in the following sentence with exactly one word for each blank then write the sentence with the filled words completely. Remember just sentence without any other thing and the number of filled words equal to the number of blanks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=500,  \n",
    "                temperature=0,\n",
    "                n=1,\n",
    "                stop=None\n",
    "            )\n",
    "            # Extract the assistant's reply from the response\n",
    "            filled_prompt = response['choices'][0]['message']['content'].strip()\n",
    "            return filled_prompt\n",
    "\n",
    "\n",
    "    def get_completion(\n",
    "        self,\n",
    "        messages: list[dict[str, str]],\n",
    "        model: str = \"gpt-4\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        stop=None,\n",
    "        seed=123,\n",
    "        tools=None,\n",
    "        logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "        top_logprobs=None,\n",
    "    ) -> str:\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop\": stop,\n",
    "            \"seed\": seed,\n",
    "            \"logprobs\": logprobs,\n",
    "            \"top_logprobs\": top_logprobs,\n",
    "        }\n",
    "        if tools:\n",
    "            params[\"tools\"] = tools\n",
    "\n",
    "        completion = openai.ChatCompletion.create(**params)\n",
    "        return completion\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__ __ Launches __ to Enhance Urban __ __.', 'Local Mayor Launches __ to Enhance __ __ Transport.', 'Local __ Launches Initiative to Enhance Urban Public Transport.', 'Local __ Launches __ to Enhance Urban __ __.', 'Local Mayor Launches __ to Enhance Urban __ Transport.', '__ __ __ __ __ __ Urban __ __.', 'Local Mayor Launches Initiative to __ __ Public __.', 'Local Mayor __ Initiative to Enhance __ Public Transport.', '__ __ __ Initiative to Enhance __ Public __.', '__ __ Launches __ __ __ __ __ __.', 'Local Mayor __ __ __ Enhance Urban Public __.', 'Local __ Launches Initiative to Enhance Urban Public Transport.', 'Local Mayor Launches Initiative __ __ __ __ Transport.', '__ __ Launches __ __ __ __ __ __.', '__ __ Launches Initiative to Enhance Urban Public Transport.', '__ __ __ __ __ Enhance Urban __ __.', '__ Mayor Launches __ __ Enhance __ Public Transport.', '__ __ __ __ __ Enhance __ Public Transport.', '__ __ __ __ __ __ __ __ __.', '__ __ __ __ __ __ __ __ __.', '__ __ Launches Initiative __ __ __ __ Transport.', 'Local __ __ Initiative to __ Urban __ __.', 'Local __ __ Initiative __ __ __ Public __.', 'Local __ __ __ __ __ __ Public __.', 'Local Mayor Launches Initiative to __ Urban __ Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.', 'Local Mayor Launches Initiative to Enhance __ Public Transport.', '__ __ __ __ __ __ __ __ __.', 'Local Mayor __ Initiative to __ __ Public __.', '__ __ __ __ to __ __ __ Transport.', 'Local __ Launches Initiative to __ __ __ __.', 'Local __ Launches __ to __ Urban Public Transport.', 'Local __ Launches Initiative __ Enhance __ __ __.', 'Local Mayor Launches Initiative to Enhance Urban __ Transport.', '__ Mayor Launches __ __ __ __ __ __.', '__ Mayor __ __ to __ __ __ __.', '__ __ __ Initiative to __ __ Public Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.', 'Local __ __ __ to __ __ Public __.', '__ __ Launches Initiative __ __ __ __ Transport.', 'Local Mayor __ __ __ __ __ Public __.', '__ __ __ __ __ __ Urban __ __.', 'Local Mayor Launches Initiative __ Enhance Urban Public Transport.', 'Local Mayor Launches __ __ __ __ Public Transport.', '__ Mayor Launches Initiative to Enhance __ Public Transport.', '__ __ Launches __ to Enhance Urban Public __.', '__ __ __ Initiative __ Enhance Urban __ __.', 'Local __ __ Initiative to __ __ Public Transport.', 'Local __ Launches __ to __ __ Public __.', 'Local Mayor Launches Initiative to Enhance Urban Public __.']\n",
      "['Technology', 'Politics', 'Technology', 'Technology', 'Politics', 'Others', 'Politics', 'Politics', 'Politics', 'Technology', 'Politics', 'Technology', 'Politics', 'Technology', 'Technology', 'Technology', 'Politics', 'Technology', 'Others', 'Others', 'Technology', 'Others', 'Others', 'Others', 'Politics', 'Politics', 'Politics', 'others', 'Politics', 'Technology', 'Others', 'Technology', 'Others', 'Politics', 'Politics', 'Politics', 'Technology', 'Politics', 'Others', 'Technology', 'Politics', 'Others', 'Politics', 'Politics', 'Politics', 'Technology', 'Technology', 'Technology', 'Technology', 'Politics']\n"
     ]
    }
   ],
   "source": [
    " #  suppose this two words Photo-Editing \n",
    "\n",
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]\n",
    "original_prompt = headlines[1]\n",
    "prompt_generator = Prompt_Generator(prompt=original_prompt, LLM_Handler=LLM_Handler() ,num_datasets = 50 , pval=np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "\n",
    "# word_to_index, _, N = prompt_generator.build_vocabulary(original_prompt)\n",
    "# baseline_list = [[] for _ in range(N)]\n",
    "# rng = np.random.default_rng()\n",
    "# num_iterations = 1\n",
    "\n",
    "# for _ in range(num_iterations):\n",
    "#     p = np.random.choice(np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "#     # print(p)\n",
    "\n",
    "#     modified_prompt = remove_words_with_probability(original_prompt, probability=p)\n",
    "#     # print(modified_prompt)\n",
    "#     # Get the LLM to fill in the blanks\n",
    "#     filled_prompt = prompt_generator.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "#     # print(\"filled prompt: \", filled_prompt)\n",
    "#     filled_words = re.findall(r'\\b\\w+\\b', filled_prompt)\n",
    "\n",
    " \n",
    "#     modified_words = word_pattern.findall(modified_prompt)\n",
    "#     for idx, token in enumerate(modified_words):\n",
    "#         if token == '__':\n",
    "#             # print(\"fghj\",filled_words[idx])\n",
    "#             baseline_list[idx].append(filled_words[idx])\n",
    "\n",
    "\n",
    "# baseline_list = prompt_generator.create_baseline_words()\n",
    "\n",
    "X = prompt_generator.create_X()\n",
    "print(prompt_generator.sample_prompts)\n",
    "y, outputs = prompt_generator.create_y(prompt_generator.sample_prompts)\n",
    "y = np.exp(y)\n",
    "print(outputs)\n",
    "\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "Optimal lambda: 0.01599858719606059\n",
      "Coefficients: [0.         0.0119888  0.         0.         0.         0.\n",
      " 0.         0.         0.02903531]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "alpha_values = np.logspace(-4, 5, 50)\n",
    "lasso_cv = LassoCV(alphas = alpha_values ,cv=5, random_state=123).fit(X, y)\n",
    "print(original_prompt)\n",
    "best_lambda = lasso_cv.alpha_\n",
    "print(\"Optimal lambda:\", best_lambda)\n",
    "\n",
    "# print(\"Coefficients:\", lasso_cv.coef_)\n",
    "\n",
    "print(\"Coefficients:\", lasso_cv.coef_ * np.sqrt(prompt_generator.coef_scaling()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Solver\n",
    "lasso_solver = Solver.SparseLassoSolver(coef_scaling= prompt_generator.coef_scaling())\n",
    "# X = prompt_generator.create_X()\n",
    "# print(prompt_generator.sample_prompts)\n",
    "# y, outputs = prompt_generator.create_y(prompt_generator.sample_prompts)\n",
    "res = lasso_solver.fit(X, y)\n",
    "print(\"Coefficients:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 2.8411709705692926e-05\n",
      "Coefficients: [ 0.02560887  0.07667842  0.0062007   0.01459472 -0.00722834  0.05050578\n",
      "  0.02832886  0.03396336  0.08356675]\n"
     ]
    }
   ],
   "source": [
    "import Solver\n",
    "lasso_solver = Solver.LassoSolver(coef_scaling= prompt_generator.coef_scaling())\n",
    "# X = prompt_generator.create_X()\n",
    "# print(prompt_generator.sample_prompts)\n",
    "# y, outputs = prompt_generator.create_y(prompt_generator.sample_prompts)\n",
    "res = lasso_solver.fit(X, y)\n",
    "print(\"Coefficients:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SourceCATELearner:\n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize learner for source-wise CATE estimation\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.propensity_model = LogisticRegression(random_state=random_state)\n",
    "        self.outcome_model = LinearRegression()\n",
    "        self.final_model = LinearRegression()\n",
    "        \n",
    "    def estimate_single_source_cate(self, X, y, source_idx):\n",
    "        \"\"\"\n",
    "        Estimate CATE for a single source\n",
    "        \n",
    "        Parameters:\n",
    "        X (np.array): M x n binary matrix where M is number of samples, n is number of sources\n",
    "        y (np.array): Target variable for each sample\n",
    "        source_idx (int): Index of the source to estimate CATE for\n",
    "        \n",
    "        Returns:\n",
    "        float: CATE estimate for the specified source\n",
    "        \"\"\"\n",
    "        # Extract treatment for the specific source\n",
    "        T = X[:, source_idx]\n",
    "        \n",
    "        # Create features excluding the target source\n",
    "        X_reduced = np.delete(X, source_idx, axis=1)\n",
    "        \n",
    "        # Step 1: Estimate propensity score e(X) = P(T=1|X)\n",
    "        self.propensity_model.fit(X_reduced, T)\n",
    "        e_x = self.propensity_model.predict_proba(X_reduced)[:, 1]\n",
    "        \n",
    "        # Step 2: Estimate outcome model m(X)\n",
    "        self.outcome_model.fit(X_reduced, y)\n",
    "        m_x = self.outcome_model.predict(X_reduced)\n",
    "        \n",
    "        # Step 3: Calculate pseudo-outcome\n",
    "        pseudo_outcome = (y - m_x) / (T - e_x)\n",
    "        \n",
    "        # Step 4: Final regression for CATE\n",
    "        self.final_model.fit(X_reduced, pseudo_outcome)\n",
    "        \n",
    "        # Return average CATE\n",
    "        return np.mean(self.final_model.predict(X_reduced))\n",
    "    \n",
    "    def estimate_all_sources_cate(self, X, y):\n",
    "        \"\"\"\n",
    "        Estimate CATE for all sources simultaneously\n",
    "        \n",
    "        Parameters:\n",
    "        X (np.array): M x n binary matrix\n",
    "        y (np.array): Target variable for each sample\n",
    "        \n",
    "        Returns:\n",
    "        np.array: CATE estimates for all sources\n",
    "        \"\"\"\n",
    "        n_sources = X.shape[1]\n",
    "        cates = np.zeros(n_sources)\n",
    "        \n",
    "        # Method 1: Sequential estimation\n",
    "        for i in range(n_sources):\n",
    "            cates[i] = self.estimate_single_source_cate(X, y, i)\n",
    "            \n",
    "        return cates\n",
    "\n",
    "\n",
    "\n",
    "# Test implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate test data\n",
    "    M, n = 1000, 5  # M samples, n sources\n",
    "    X, y, true_cates = generate_test_data(M, n)\n",
    "    \n",
    "    # Initialize and test learner\n",
    "    learner = SourceCATELearner()\n",
    "    \n",
    "    # Test single source CATE\n",
    "    print(\"\\nTesting single source CATE estimation:\")\n",
    "    source_idx = 0\n",
    "    single_cate = learner.estimate_single_source_cate(X, y, source_idx)\n",
    "    print(f\"Estimated CATE for source {source_idx}: {single_cate:.3f}\")\n",
    "    print(f\"True CATE for source {source_idx}: {true_cates[source_idx]:.3f}\")\n",
    "    \n",
    "    # Test all sources CATE\n",
    "    print(\"\\nTesting all sources CATE estimation:\")\n",
    "    estimated_cates = learner.estimate_all_sources_cate(X, y)\n",
    "    print(\"Estimated CATEs:\", estimated_cates)\n",
    "    print(\"True CATEs:\", true_cates)\n",
    "    print(\"\\nMean Absolute Error:\", np.mean(np.abs(estimated_cates - true_cates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Training Mean Squared Error (MSE): 22.26310886203446\n",
      "OLS Training Mean Absolute Error (MAE): 3.1960798147776086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X, y)\n",
    "predictions_ols_train = ols_model.predict(X)\n",
    "mse_ols_train = mean_squared_error(y, predictions_ols_train)\n",
    "print(\"OLS Training Mean Squared Error (MSE):\", mse_ols_train)\n",
    "\n",
    "mae_ols_train = mean_absolute_error(y, predictions_ols_train)\n",
    "print(\"OLS Training Mean Absolute Error (MAE):\", mae_ols_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "load_dotenv('config.env')\n",
    "hf_auth_token  = os.environ.get(\"HF_API_KEY\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Example: a restricted-access Llama 3 model\n",
    "\n",
    "# Supply the access token to each from_pretrained call.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_auth_token=hf_auth_token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  \n",
    "    use_auth_token=hf_auth_token\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
