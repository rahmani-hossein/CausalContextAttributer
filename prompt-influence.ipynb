{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class Prompt_Generator():\n",
    "\n",
    "\n",
    "    def __init__(self, prompt, LLM_Handler, num_datasets = 5 , pval=np.array([0.2, 0.4, 0.6, 0.8])):\n",
    "        \"\"\"\n",
    "         Parameters:\n",
    "        self.original_prompt : original prompt to create subsets\n",
    "        self. pval : distribution P that p is drawn from. Normally  pval=[0.2, 0.4, 0.6, 0.8]\n",
    "        self.num_datasets : Number of datasets or subsets (M in the code) \n",
    "        self.prompts : a list of prompts that are our datasets.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.original_prompt = prompt\n",
    "        self. pval = pval\n",
    "        self.num_datasets = num_datasets\n",
    "        self.sample_prompts = []\n",
    "        self.LLM_Handler = LLM_Handler\n",
    "        # self.prompts.append(self.original_prompt) the original prompt itself isn't something special.\n",
    "\n",
    "\n",
    "    def sample_p(self, size=None):\n",
    "        return np.random.choice(self.pval, size=size)\n",
    "\n",
    "\n",
    "    def coef_scaling(self):\n",
    "        return (1/(self.pval *(1-self.pval))).mean()\n",
    "\n",
    "    def build_vocabulary(self, originalprompt):\n",
    "        \"\"\"\n",
    "        parameter : original prompt\n",
    "\n",
    "        return:\n",
    "        word_to_index : a dic to map words to index inthe original prompt.\n",
    "        unique_words : unique words in the original prompt. If a word is repetitive, the first index will be saved for word_to_index.\n",
    "        N : number of unique words  \n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', originalprompt)\n",
    "        # Step 2: Build the vocabulary\n",
    "        unique_words = []\n",
    "        word_to_index = {}\n",
    "        for word in words:\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(unique_words)\n",
    "                unique_words.append(word)\n",
    "\n",
    "        N = len(unique_words)\n",
    "        return word_to_index, unique_words, N\n",
    "    \n",
    "\n",
    "# To Do: resolve the bug that chatgpt doesn't produce exactly that much words.\n",
    "    def create_baseline_words(self):\n",
    "        word_to_index, _, N = self.build_vocabulary(self.original_prompt)\n",
    "        baseline_list = [[] for _ in range(N)]\n",
    "        rng = np.random.default_rng()\n",
    "        num_iterations = 5   #100 *N\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            p = np.random.choice(np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "            # print(p)\n",
    "            modified_prompt = remove_words_with_probability(self.original_prompt, probability=p)\n",
    "            # print(modified_prompt)\n",
    "            # Get the LLM to fill in the blanks\n",
    "            filled_prompt = self.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "            # print(\"filled prompt: \", filled_prompt)\n",
    "            filled_words = re.findall(r'\\b\\w+\\b', filled_prompt)\n",
    "\n",
    "        \n",
    "            modified_words = word_pattern.findall(modified_prompt)\n",
    "            for idx, token in enumerate(modified_words):\n",
    "                if token == '__':\n",
    "                    # print(\"fghj\",filled_words[idx])\n",
    "                    baseline_list[idx].append(filled_words[idx])\n",
    "\n",
    "\n",
    "        \n",
    "        return baseline_list\n",
    "\n",
    "\n",
    "\n",
    "#To Do : might change even before p-featurization\n",
    "    def create_X(self, method='uncompleted'):\n",
    "        self.ps = []\n",
    "        word_to_index, unique_words, N  = self.build_vocabulary(self.original_prompt)\n",
    "        X = np.zeros((self.num_datasets, N)) \n",
    "        nu = self.coef_scaling() # variance of each feature\n",
    "        for m in range(self.num_datasets):\n",
    "            p = self.sample_p()\n",
    "            self.ps.append(p)\n",
    "            modified_prompt = remove_words_with_probability(self.original_prompt, probability= p)\n",
    "            X[m,:] = -1/((1-p)* np.sqrt(nu))\n",
    "            modified_promptWords = re.findall(r'\\b\\w+\\b', modified_prompt)\n",
    "            for modified_word in modified_promptWords:\n",
    "                if modified_word !=\"__\":\n",
    "                    # print(\"we change probability of word: \", modified_word, \"in dataset \", m, \" with index \", word_to_index[modified_word])\n",
    "                    X[m,word_to_index[modified_word]] = 1/(p* np.sqrt(nu))\n",
    "            \n",
    "            # completed_prompt = self.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "            self.sample_prompts.append(modified_prompt)\n",
    "        return X\n",
    "    \n",
    "    # To Do sample from baseline for each word for putting typical word in it.\n",
    "    def fill_prompt_form_baseline(self, prompts):\n",
    "\n",
    "        return prompts\n",
    "    \n",
    "    \n",
    "    # To do: add the baseline method. Here is just uncompleted.\n",
    "    def create_y(self, prompts, method='uncomplete'):\n",
    "        CLASSIFICATION_PROMPT_Base = \"\"\"You will be given a headline of a news article.\n",
    "        Classify the article into one of the following categories: Technology, Politics, Sports, Art or others.\n",
    "        Return only the name of the category, and nothing else.\n",
    "        MAKE SURE your output is one of the four categories stated.\n",
    "        Article headline: {prompt}\"\"\"\n",
    "\n",
    "        CLASSIFICATION_PROMPT_Method2 = \"\"\"You will be given a headline of a news article with some blanks instead of words.\n",
    "        Classify the article into one of the following categories: Technology, Politics, Sports, Art or others.\n",
    "        Return only the name of the category, and nothing else.\n",
    "        MAKE SURE your output is one of the four categories stated.\n",
    "        Article headline: {prompt}\"\"\"\n",
    "\n",
    "        if method == 'baseline':\n",
    "            baseline_list = self.create_baseline_words()\n",
    "            self.sample_prompts = self.fill_prompt_form_baseline(prompts) #TO Do\n",
    "\n",
    "        y = np.zeros(len(prompts))\n",
    "        outputs = []\n",
    "        for i in range(len(prompts)):\n",
    "            if method=='baseline':\n",
    "                API_RESPONSE = self.LLM_Handler.get_completion(\n",
    "                [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT_Base.format(prompt=prompts[i])}],\n",
    "                model=\"gpt-4\",\n",
    "                logprobs=True,\n",
    "                top_logprobs=1,\n",
    "                )\n",
    "            elif method =='uncomplete':\n",
    "                API_RESPONSE = self.LLM_Handler.get_completion(\n",
    "                [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT_Method2.format(prompt=prompts[i])}],\n",
    "                model=\"gpt-4\",\n",
    "                logprobs=True,\n",
    "                top_logprobs=1,\n",
    "                )\n",
    "\n",
    "\n",
    "            \n",
    "            val = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs[0]\n",
    "            outputs.append(val.token)\n",
    "            y[i] = val.logprob\n",
    "\n",
    "        return y, outputs\n",
    "        \n",
    "    \n",
    "\n",
    "          \n",
    "# Pre-compile regex patterns for efficiency\n",
    "\"\"\"\n",
    "word_pattern matches complete words composed of alphanumeric characters.\n",
    "token_pattern splits the prompt into words, whitespace, and punctuation, ensuring that all characters\n",
    " (including spaces, punctuation like commas and periods, and newlines) are preserved.\n",
    "\"\"\"\n",
    "word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "token_pattern = re.compile(r'\\b\\w+\\b|\\s+|[^\\w\\s]')\n",
    "\n",
    "def remove_words_with_probability(prompt, probability=0.8):\n",
    "    tokens = token_pattern.findall(prompt)\n",
    "    modified_tokens = [\n",
    "        '__' if word_pattern.fullmatch(token) and np.random.uniform(0,1) > probability else token\n",
    "        for token in tokens\n",
    "    ]\n",
    "    return ''.join(modified_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM_Handler():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv('config.env')\n",
    "        openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    \n",
    "    def fill_in_blanks(self, prompt):\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Please fill in the blanks in the following sentence with exactly one word for each blank then write the sentence with the filled words completely. Remember just sentence without any other thing and the number of filled words equal to the number of blanks.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=500,  \n",
    "                temperature=0,\n",
    "                n=1,\n",
    "                stop=None\n",
    "            )\n",
    "            # Extract the assistant's reply from the response\n",
    "            filled_prompt = response['choices'][0]['message']['content'].strip()\n",
    "            return filled_prompt\n",
    "\n",
    "\n",
    "    def get_completion(\n",
    "        self,\n",
    "        messages: list[dict[str, str]],\n",
    "        model: str = \"gpt-4\",\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        stop=None,\n",
    "        seed=123,\n",
    "        tools=None,\n",
    "        logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "        top_logprobs=None,\n",
    "    ) -> str:\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop\": stop,\n",
    "            \"seed\": seed,\n",
    "            \"logprobs\": logprobs,\n",
    "            \"top_logprobs\": top_logprobs,\n",
    "        }\n",
    "        if tools:\n",
    "            params[\"tools\"] = tools\n",
    "\n",
    "        completion = openai.ChatCompletion.create(**params)\n",
    "        return completion\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Local Mayor Launches __ __ Enhance __ Public Transport.', 'Local __ Launches __ __ Enhance __ __ Transport.', '__ __ __ __ __ __ __ __ Transport.', '__ __ __ Initiative __ Enhance __ Public __.', 'Local Mayor Launches Initiative to __ Urban __ __.', 'Local Mayor Launches Initiative to Enhance Urban __ Transport.', 'Local Mayor __ Initiative to Enhance Urban Public Transport.', '__ __ __ __ __ __ __ __ __.', 'Local Mayor __ Initiative to Enhance Urban Public Transport.', 'Local Mayor __ Initiative to __ Urban Public Transport.', '__ __ __ __ to __ __ Public __.', '__ Mayor Launches Initiative to __ __ Public __.', 'Local __ __ __ __ __ __ __ Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public __.', 'Local __ Launches Initiative to Enhance Urban Public Transport.', '__ Mayor Launches __ __ Enhance Urban __ __.', '__ Mayor __ __ __ __ Urban __ Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.', '__ Mayor Launches __ to __ __ __ __.', 'Local Mayor Launches Initiative __ __ Urban Public Transport.', 'Local Mayor Launches __ to __ Urban Public __.', '__ __ __ __ __ __ __ __ __.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.', '__ __ Launches __ __ __ __ Public Transport.', 'Local Mayor Launches __ __ __ __ __ __.', 'Local Mayor Launches __ to Enhance Urban Public Transport.', '__ Mayor __ __ __ __ __ __ Transport.', '__ __ __ Initiative __ __ __ __ __.', '__ Mayor __ __ to Enhance Urban __ Transport.', 'Local Mayor Launches Initiative __ Enhance __ __ Transport.', 'Local Mayor __ Initiative __ Enhance Urban Public Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public __.', '__ __ __ __ to __ __ Public Transport.', 'Local Mayor __ __ to __ __ __ __.', '__ __ __ __ __ Enhance __ __ Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.', '__ __ __ Initiative __ __ __ __ __.', '__ Mayor Launches Initiative to __ Urban __ Transport.', '__ __ __ __ __ __ Urban __ __.', 'Local __ Launches Initiative __ __ Urban __ __.', '__ __ Launches __ __ Enhance Urban __ __.', 'Local __ Launches __ to Enhance Urban __ Transport.', '__ Mayor Launches Initiative to Enhance Urban __ Transport.', '__ __ __ __ __ __ __ __ __.', 'Local Mayor Launches Initiative __ Enhance Urban Public Transport.', '__ __ __ __ __ __ __ __ __.', 'Local Mayor Launches Initiative __ Enhance __ Public Transport.', '__ __ __ Initiative __ Enhance __ __ __.', '__ Mayor Launches Initiative to __ __ __ Transport.', 'Local Mayor Launches Initiative to Enhance Urban Public Transport.']\n",
      "['Politics', 'Technology', 'Technology', 'Politics', 'Politics', 'Politics', 'Politics', 'Others', 'Politics', 'Politics', 'Others', 'Politics', 'Technology', 'Politics', 'Technology', 'Politics', 'Politics', 'Politics', 'Politics', 'Politics', 'Politics', 'Others', 'Politics', 'Technology', 'Politics', 'Politics', 'Politics', 'Others', 'Politics', 'Politics', 'Politics', 'Politics', 'Technology', 'Politics', 'Technology', 'Politics', 'Others', 'Politics', 'Others', 'Others', 'Technology', 'Technology', 'Politics', 'others', 'Politics', 'Others', 'Politics', 'Others', 'Politics', 'Politics']\n"
     ]
    }
   ],
   "source": [
    " #  suppose this two words Photo-Editing \n",
    "\n",
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]\n",
    "original_prompt = headlines[1]\n",
    "prompt_generator = Prompt_Generator(prompt=original_prompt, LLM_Handler=LLM_Handler() ,num_datasets = 50 , pval=np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "\n",
    "# word_to_index, _, N = prompt_generator.build_vocabulary(original_prompt)\n",
    "# baseline_list = [[] for _ in range(N)]\n",
    "# rng = np.random.default_rng()\n",
    "# num_iterations = 1\n",
    "\n",
    "# for _ in range(num_iterations):\n",
    "#     p = np.random.choice(np.array([0.2, 0.4, 0.6, 0.8]))\n",
    "#     # print(p)\n",
    "\n",
    "#     modified_prompt = remove_words_with_probability(original_prompt, probability=p)\n",
    "#     # print(modified_prompt)\n",
    "#     # Get the LLM to fill in the blanks\n",
    "#     filled_prompt = prompt_generator.LLM_Handler.fill_in_blanks(modified_prompt)\n",
    "#     # print(\"filled prompt: \", filled_prompt)\n",
    "#     filled_words = re.findall(r'\\b\\w+\\b', filled_prompt)\n",
    "\n",
    " \n",
    "#     modified_words = word_pattern.findall(modified_prompt)\n",
    "#     for idx, token in enumerate(modified_words):\n",
    "#         if token == '__':\n",
    "#             # print(\"fghj\",filled_words[idx])\n",
    "#             baseline_list[idx].append(filled_words[idx])\n",
    "\n",
    "\n",
    "# baseline_list = prompt_generator.create_baseline_words()\n",
    "\n",
    "X = prompt_generator.create_X()\n",
    "print(prompt_generator.sample_prompts)\n",
    "y, outputs = prompt_generator.create_y(prompt_generator.sample_prompts)\n",
    "y = np.exp(y)\n",
    "print(outputs)\n",
    "\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.54772256, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256, -0.54772256,  2.19089023],\n",
       "       [ 1.09544512, -0.73029674,  1.09544512, -0.73029674,  1.09544512,\n",
       "        -0.73029674, -0.73029674, -0.73029674, -0.73029674],\n",
       "       [-2.19089023,  0.54772256, -2.19089023, -2.19089023,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 0.54772256, -2.19089023,  0.54772256,  0.54772256,  0.54772256,\n",
       "        -2.19089023,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 2.19089023, -0.54772256,  2.19089023, -0.54772256, -0.54772256,\n",
       "         2.19089023,  2.19089023, -0.54772256, -0.54772256],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 0.54772256, -2.19089023,  0.54772256,  0.54772256,  0.54772256,\n",
       "        -2.19089023, -2.19089023,  0.54772256,  0.54772256],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256,  2.19089023,\n",
       "        -0.54772256, -0.54772256, -0.54772256, -0.54772256],\n",
       "       [ 0.54772256,  0.54772256, -2.19089023, -2.19089023,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 0.73029674, -1.09544512, -1.09544512, -1.09544512, -1.09544512,\n",
       "         0.73029674,  0.73029674, -1.09544512,  0.73029674],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256, -2.19089023],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 1.09544512, -0.73029674,  1.09544512,  1.09544512, -0.73029674,\n",
       "        -0.73029674, -0.73029674, -0.73029674, -0.73029674],\n",
       "       [-0.73029674, -0.73029674, -0.73029674, -0.73029674, -0.73029674,\n",
       "         1.09544512, -0.73029674,  1.09544512, -0.73029674],\n",
       "       [-1.09544512,  0.73029674,  0.73029674, -1.09544512,  0.73029674,\n",
       "        -1.09544512,  0.73029674,  0.73029674,  0.73029674],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256, -2.19089023,  0.54772256,  0.54772256],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "        -2.19089023,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [-1.09544512,  0.73029674,  0.73029674,  0.73029674,  0.73029674,\n",
       "        -1.09544512, -1.09544512,  0.73029674,  0.73029674],\n",
       "       [-0.73029674,  1.09544512,  1.09544512, -0.73029674,  1.09544512,\n",
       "        -0.73029674,  1.09544512, -0.73029674, -0.73029674],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256, -2.19089023,  0.54772256,  0.54772256],\n",
       "       [-0.73029674, -0.73029674, -0.73029674,  1.09544512,  1.09544512,\n",
       "        -0.73029674, -0.73029674,  1.09544512, -0.73029674],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256,  2.19089023,\n",
       "        -0.54772256, -0.54772256,  2.19089023, -0.54772256],\n",
       "       [ 0.54772256,  0.54772256, -2.19089023,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 1.09544512,  1.09544512,  1.09544512, -0.73029674, -0.73029674,\n",
       "         1.09544512, -0.73029674,  1.09544512, -0.73029674],\n",
       "       [ 0.73029674, -1.09544512,  0.73029674, -1.09544512,  0.73029674,\n",
       "         0.73029674,  0.73029674,  0.73029674,  0.73029674],\n",
       "       [-1.09544512,  0.73029674,  0.73029674,  0.73029674,  0.73029674,\n",
       "         0.73029674,  0.73029674,  0.73029674,  0.73029674],\n",
       "       [-2.19089023,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [-0.54772256, -0.54772256,  2.19089023, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256, -0.54772256,  2.19089023],\n",
       "       [ 0.73029674, -1.09544512, -1.09544512,  0.73029674, -1.09544512,\n",
       "        -1.09544512,  0.73029674, -1.09544512,  0.73029674],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256,  2.19089023,\n",
       "        -0.54772256, -0.54772256, -0.54772256, -0.54772256],\n",
       "       [ 2.19089023, -0.54772256,  2.19089023, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256,  2.19089023, -0.54772256],\n",
       "       [-0.73029674, -0.73029674, -0.73029674, -0.73029674,  1.09544512,\n",
       "         1.09544512, -0.73029674, -0.73029674,  1.09544512],\n",
       "       [ 0.73029674,  0.73029674,  0.73029674,  0.73029674,  0.73029674,\n",
       "        -1.09544512, -1.09544512,  0.73029674,  0.73029674],\n",
       "       [-0.73029674,  1.09544512,  1.09544512, -0.73029674,  1.09544512,\n",
       "        -0.73029674,  1.09544512, -0.73029674, -0.73029674],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256, -0.54772256, -0.54772256],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256,  2.19089023,\n",
       "         2.19089023, -0.54772256,  2.19089023, -0.54772256],\n",
       "       [ 0.54772256,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "        -2.19089023,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [-1.09544512, -1.09544512, -1.09544512,  0.73029674,  0.73029674,\n",
       "         0.73029674,  0.73029674, -1.09544512,  0.73029674],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256,  2.19089023, -0.54772256],\n",
       "       [-0.73029674, -0.73029674, -0.73029674, -0.73029674, -0.73029674,\n",
       "        -0.73029674, -0.73029674,  1.09544512,  1.09544512],\n",
       "       [-0.73029674,  1.09544512,  1.09544512,  1.09544512, -0.73029674,\n",
       "        -0.73029674,  1.09544512, -0.73029674, -0.73029674],\n",
       "       [-0.73029674,  1.09544512,  1.09544512, -0.73029674,  1.09544512,\n",
       "        -0.73029674,  1.09544512, -0.73029674,  1.09544512],\n",
       "       [ 0.73029674,  0.73029674,  0.73029674, -1.09544512, -1.09544512,\n",
       "        -1.09544512,  0.73029674,  0.73029674, -1.09544512],\n",
       "       [ 0.73029674,  0.73029674,  0.73029674,  0.73029674,  0.73029674,\n",
       "        -1.09544512,  0.73029674, -1.09544512,  0.73029674],\n",
       "       [-2.19089023, -2.19089023,  0.54772256, -2.19089023,  0.54772256,\n",
       "        -2.19089023,  0.54772256,  0.54772256,  0.54772256],\n",
       "       [ 2.19089023, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "         2.19089023, -0.54772256, -0.54772256, -0.54772256],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256, -0.54772256, -0.54772256],\n",
       "       [-2.19089023,  0.54772256,  0.54772256,  0.54772256,  0.54772256,\n",
       "         0.54772256, -2.19089023,  0.54772256,  0.54772256],\n",
       "       [-0.54772256, -0.54772256, -0.54772256, -0.54772256, -0.54772256,\n",
       "        -0.54772256, -0.54772256, -0.54772256, -0.54772256]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "class BASE_AME_Solver(ABC):\n",
    "    \"\"\"\n",
    "    A base solver class.\n",
    "\n",
    "    Methods:\n",
    "        fit(self, masks: NDArray, outputs: NDArray, num_output_tokens: int) -> Tuple[NDArray, NDArray]:\n",
    "            Fit the solver to the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y): ...\n",
    "\n",
    "\n",
    "\n",
    "class LassoSolver(BASE_AME_Solver):\n",
    "    \"\"\"\n",
    "    A LASSO solver using the scikit-learn library for estimating AME.\n",
    "\n",
    "    Attributes:\n",
    "        lasso_alpha (float):\n",
    "            The alpha parameter for the LASSO regression. Defaults to 0.01.\n",
    "\n",
    "    Methods:\n",
    "        fit(self, masks: NDArray, outputs: NDArray, num_output_tokens: int) -> Tuple[NDArray, NDArray]:\n",
    "            Fit the solver to the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coef_scaling) -> None:\n",
    "        super().__init__()\n",
    "        self.coef_scaling = coef_scaling\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        alpha_values = np.logspace(-4, 5, 50)\n",
    "        lasso_cv = LassoCV(\n",
    "            alphas=alpha_values,\n",
    "            cv=5,\n",
    "            random_state=123\n",
    "        ).fit(X, y)\n",
    "        alpha_values = np.logspace(-4, 5, 50)\n",
    "        lasso_cv = LassoCV(alphas = alpha_values ,cv=5, random_state=123).fit(X, y)\n",
    "        best_lambda = lasso_cv.alpha_\n",
    "        print(\"Optimal lambda:\", best_lambda)\n",
    "        return lasso_cv.coef_ * np.sqrt(self.coef_scaling)\n",
    "    \n",
    "class OrthogonalSolver(BASE_AME_Solver):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(X, y, t_indexes):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "Optimal lambda: 0.006866488450042998\n",
      "Coefficients: [ 0.01229691  0.05862561  0.03755394 -0.          0.02086181  0.\n",
      "  0.02865098  0.01156562  0.05407037]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "alpha_values = np.logspace(-4, 5, 50)\n",
    "lasso_cv = LassoCV(alphas = alpha_values ,cv=5, random_state=123).fit(X, y)\n",
    "print(original_prompt)\n",
    "best_lambda = lasso_cv.alpha_\n",
    "print(\"Optimal lambda:\", best_lambda)\n",
    "\n",
    "# print(\"Coefficients:\", lasso_cv.coef_)\n",
    "\n",
    "print(\"Coefficients:\", lasso_cv.coef_ * np.sqrt(prompt_generator.coef_scaling()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 0.006866488450042998\n",
      "Coefficients: [ 0.01229691  0.05862561  0.03755394 -0.          0.02086181  0.\n",
      "  0.02865098  0.01156562  0.05407037]\n"
     ]
    }
   ],
   "source": [
    "lasso_solver = LassoSolver(coef_scaling= prompt_generator.coef_scaling())\n",
    "# X = prompt_generator.create_X()\n",
    "# print(prompt_generator.sample_prompts)\n",
    "# y, outputs = prompt_generator.create_y(prompt_generator.sample_prompts)\n",
    "res = lasso_solver.fit(X, y)\n",
    "print(\"Coefficients:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SourceCATELearner:\n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize learner for source-wise CATE estimation\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.propensity_model = LogisticRegression(random_state=random_state)\n",
    "        self.outcome_model = LinearRegression()\n",
    "        self.final_model = LinearRegression()\n",
    "        \n",
    "    def estimate_single_source_cate(self, X, y, source_idx):\n",
    "        \"\"\"\n",
    "        Estimate CATE for a single source\n",
    "        \n",
    "        Parameters:\n",
    "        X (np.array): M x n binary matrix where M is number of samples, n is number of sources\n",
    "        y (np.array): Target variable for each sample\n",
    "        source_idx (int): Index of the source to estimate CATE for\n",
    "        \n",
    "        Returns:\n",
    "        float: CATE estimate for the specified source\n",
    "        \"\"\"\n",
    "        # Extract treatment for the specific source\n",
    "        T = X[:, source_idx]\n",
    "        \n",
    "        # Create features excluding the target source\n",
    "        X_reduced = np.delete(X, source_idx, axis=1)\n",
    "        \n",
    "        # Step 1: Estimate propensity score e(X) = P(T=1|X)\n",
    "        self.propensity_model.fit(X_reduced, T)\n",
    "        e_x = self.propensity_model.predict_proba(X_reduced)[:, 1]\n",
    "        \n",
    "        # Step 2: Estimate outcome model m(X)\n",
    "        self.outcome_model.fit(X_reduced, y)\n",
    "        m_x = self.outcome_model.predict(X_reduced)\n",
    "        \n",
    "        # Step 3: Calculate pseudo-outcome\n",
    "        pseudo_outcome = (y - m_x) / (T - e_x)\n",
    "        \n",
    "        # Step 4: Final regression for CATE\n",
    "        self.final_model.fit(X_reduced, pseudo_outcome)\n",
    "        \n",
    "        # Return average CATE\n",
    "        return np.mean(self.final_model.predict(X_reduced))\n",
    "    \n",
    "    def estimate_all_sources_cate(self, X, y):\n",
    "        \"\"\"\n",
    "        Estimate CATE for all sources simultaneously\n",
    "        \n",
    "        Parameters:\n",
    "        X (np.array): M x n binary matrix\n",
    "        y (np.array): Target variable for each sample\n",
    "        \n",
    "        Returns:\n",
    "        np.array: CATE estimates for all sources\n",
    "        \"\"\"\n",
    "        n_sources = X.shape[1]\n",
    "        cates = np.zeros(n_sources)\n",
    "        \n",
    "        # Method 1: Sequential estimation\n",
    "        for i in range(n_sources):\n",
    "            cates[i] = self.estimate_single_source_cate(X, y, i)\n",
    "            \n",
    "        return cates\n",
    "\n",
    "def generate_test_data(M=1000, n=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for testing\n",
    "    \n",
    "    Parameters:\n",
    "    M (int): Number of samples\n",
    "    n (int): Number of sources\n",
    "    random_state (int): Random seed\n",
    "    \n",
    "    Returns:\n",
    "    X, y: Feature matrix and target variable\n",
    "    true_cates: True CATE values for validation\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate binary source inclusion matrix\n",
    "    X = np.random.binomial(n=1, p=0.5, size=(M, n))\n",
    "    \n",
    "    # Generate true CATEs for each source\n",
    "    true_cates = np.array([1.0, -0.5, 2.0, -1.0, 1.5])[:n]\n",
    "    \n",
    "    # Generate outcome with heterogeneous treatment effects\n",
    "    baseline = np.sum(X, axis=1) * 0.5  # baseline effect\n",
    "    treatment_effects = X @ true_cates   # source-specific effects\n",
    "    noise = np.random.normal(0, 0.1, M)\n",
    "    y = baseline + treatment_effects + noise\n",
    "    \n",
    "    return X, y, true_cates\n",
    "\n",
    "# Test implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate test data\n",
    "    M, n = 1000, 5  # M samples, n sources\n",
    "    X, y, true_cates = generate_test_data(M, n)\n",
    "    \n",
    "    # Initialize and test learner\n",
    "    learner = SourceCATELearner()\n",
    "    \n",
    "    # Test single source CATE\n",
    "    print(\"\\nTesting single source CATE estimation:\")\n",
    "    source_idx = 0\n",
    "    single_cate = learner.estimate_single_source_cate(X, y, source_idx)\n",
    "    print(f\"Estimated CATE for source {source_idx}: {single_cate:.3f}\")\n",
    "    print(f\"True CATE for source {source_idx}: {true_cates[source_idx]:.3f}\")\n",
    "    \n",
    "    # Test all sources CATE\n",
    "    print(\"\\nTesting all sources CATE estimation:\")\n",
    "    estimated_cates = learner.estimate_all_sources_cate(X, y)\n",
    "    print(\"Estimated CATEs:\", estimated_cates)\n",
    "    print(\"True CATEs:\", true_cates)\n",
    "    print(\"\\nMean Absolute Error:\", np.mean(np.abs(estimated_cates - true_cates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Training Mean Squared Error (MSE): 22.26310886203446\n",
      "OLS Training Mean Absolute Error (MAE): 3.1960798147776086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X, y)\n",
    "predictions_ols_train = ols_model.predict(X)\n",
    "mse_ols_train = mean_squared_error(y, predictions_ols_train)\n",
    "print(\"OLS Training Mean Squared Error (MSE):\", mse_ols_train)\n",
    "\n",
    "mae_ols_train = mean_absolute_error(y, predictions_ols_train)\n",
    "print(\"OLS Training Mean Absolute Error (MAE):\", mae_ols_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hossein rahmani\\PycharmProjects\\Causal-LLMProject\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "load_dotenv('config.env')\n",
    "hf_auth_token  = os.environ.get(\"HF_API_KEY\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Example: a restricted-access Llama 3 model\n",
    "\n",
    "# Supply the access token to each from_pretrained call.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_auth_token=hf_auth_token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  \n",
    "    use_auth_token=hf_auth_token\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
